{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create methods that will batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class that will do batching for the algorithm\n",
    "class Audiobooks_Data_Reader():\n",
    "    # Dataset is a mandatory argument, while batch_size is optional\n",
    "    def __init__(self, dataset, batch_size=None):\n",
    "    \n",
    "        # The dataset that loads is one of 'train', 'val', or 'test'\n",
    "        npz = np.load('Audiobooks_data_{0}.npz'.format(dataset))\n",
    "        \n",
    "        # Two variables that take the values of the inputs and targets\n",
    "        self.inputs, self.targets = npz['inputs'].astype(float), npz['targets'].astype(int)\n",
    "        \n",
    "        # Counts the batch number, given the size you feed it later\n",
    "        # If batch_size is None we are either validating or testing\n",
    "        if batch_size is None:\n",
    "            self.batch_size = self.inputs.shape[0]\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.curr_batch = 0\n",
    "        self.batch_count = self.inputs.shape[0] // self.batch_size\n",
    "        \n",
    "    \n",
    "    # A method which loads the next batch\n",
    "    def __next__(self):\n",
    "        if self.curr_batch >= self.batch_count:\n",
    "            self.curr_batch = 0\n",
    "            raise StopIteration()\n",
    "            \n",
    "            \n",
    "        # You slice the dataset in batches and then the 'next' function \n",
    "        # loads them one after the other\n",
    "        batch_slice = slice(self.curr_batch * self.batch_size, (self.curr_batch + 1) * self.batch_size)\n",
    "        inputs_batch = self.inputs[batch_slice]\n",
    "        targets_batch = self.targets[batch_slice]\n",
    "        self.curr_batch += 1\n",
    "        \n",
    "        \n",
    "        # One-hot encode the targets. In this example it's a bit unnecessary\n",
    "        # since we have a 0/1 column as a target already but here's the code\n",
    "        # anyway, as it'll be useful in future\n",
    "        classes_num = 2\n",
    "        targets_one_hot = np.zeros((targets_batch.shape[0], classes_num))\n",
    "        targets_one_hot[range(targets_batch.shape[0]), targets_batch] = 1\n",
    "        \n",
    "        # The function will return the inputs batch and the one-hot encoded targets\n",
    "        return inputs_batch, targets_one_hot\n",
    "    \n",
    "    \n",
    "    # A method for iterating over the batches, as we will put them in a loop\n",
    "    # This tells Python that the class we're defining is iterable\n",
    "    # An iterator in Python is a class with the method __next__ and __iter__\n",
    "    # that defines exactly how to iterate through its objects\n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of above\n",
    "- This class has methods init, next, and iter\n",
    "- The **init** method loads the data from the .npz\n",
    "- The **next** method loads the next batch from the .npz\n",
    "- The **iter** tells Python that the class is iterable\n",
    "\n",
    "This class is an iterator. An iterator is a class with methods: **next** and **iter**. Knowing this once a variable is an instance of the class, (e.g. train_data is an instance of this class) when included in the loop it will load the first batch. Then it will iterate over the dataset taking one batch after the other until the dataset is exhausted.\n",
    "\n",
    "\n",
    "\n",
    "### init\n",
    "- **self** is Python notation that defines the method as an instance method (see below)\n",
    "This means init has only two real arguments:\n",
    "- **dataset**\n",
    "- **batch_size** is an optional argument, if we don't include it when calling the class, the class will assume it has to load all the data as a single batch\n",
    "\n",
    "Example: Let x be an instance of Audiobooks_Data_Reader, where \\\n",
    "x = Audiobooks_Data_Reader('train', 5). \\\n",
    "Result: Load the data from Audiobooks_data_train.npz, take batches of 5 samples at a time.\n",
    "\n",
    "Example: Let y be an instance of Audiobooks_Data_Reader, where \\\n",
    "y = Audiobooks_Data_Reader('val') \\\n",
    "Result: Load the data from Audiobooks_data_val.npz, take the whole dataset in a single batch.\n",
    "\n",
    "### next\n",
    "- This function slices the next batch out of the dataset and loads it for the next iteration\n",
    "- This is also where we one-hot encode the targets, the targets change from 0 to \\[1,0\\] and from 1 to \\[0,1\\]. \n",
    "- The function will return the inputs batch and the one-hot encoded targets, this is also the only output we have associated with this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static vs Instance Methods\n",
    "Imagine you have a class Person and an object that is instance of this class, e.g. Alice.\n",
    "\n",
    "A static method .sayhi(), defined without self, would be called as: Person.sayhi(). This can be taken as a property of people in general and not just a particular individual, (i.e. a property of the class and not just of the instance)\n",
    "\n",
    "One defined with self would be called as Alice.sayhi(), this is a property of the instance and not the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_size = 50\n",
    "\n",
    "tf.compat.v1.reset_default_graph() \n",
    "\n",
    "# tf.reset_default_graph() clears the memory of all variables left\n",
    "# from previous runs (reset the computational graph)\n",
    "\n",
    "inputs = tf.compat.v1.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.compat.v1.placeholder(tf.int32, [None, output_size])\n",
    "\n",
    "weights_1 = tf.compat.v1.get_variable('weights_1', [input_size, hidden_size])\n",
    "biases_1 = tf.compat.v1.get_variable('biases_1', [hidden_size])\n",
    "\n",
    "# tf.get_variable('name', shape) is a function used to declare \n",
    "# variables. The default initializer is Xavier (Glorot)\n",
    "\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "# tf.nn is a module that contains neural network support. Among \n",
    "# other things, it contains the most commonly used activation\n",
    "# functions\n",
    "\n",
    "weights_2 = tf.compat.v1.get_variable('weights_2', [hidden_size, hidden_size])\n",
    "biases_2 = tf.compat.v1.get_variable('biases_2', [hidden_size])\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "weights_3 = tf.compat.v1.get_variable('weights_3', [hidden_size, output_size])\n",
    "biases_3 = tf.compat.v1.get_variable('biases_3', [output_size])\n",
    "\n",
    "outputs = tf.matmul(outputs_2, weights_3) + biases_3\n",
    "\n",
    "# It is common practice to incorporate the final activation in\n",
    "# the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ciara\\AppData\\Local\\Temp\\ipykernel_16876\\1427916103.py:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Objective function\n",
    "loss = tf.compat.v1.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
    "\n",
    "# tf.nn.softmax_cross_entropy_with_logits(logits, labels) is a \n",
    "# function that applies a softmax activation and calculates a\n",
    "# cross-entropy loss\n",
    "\n",
    "mean_loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "optimize = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Accuracy\n",
    "out_equals_target = tf.equal(tf.argmax(outputs,axis=1), tf.argmax(targets,axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for execution\n",
    "sess = tf.compat.v1.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing variables\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching\n",
    "batch_size = 100\n",
    "\n",
    "# Early stopping\n",
    "max_epochs = 50\n",
    "\n",
    "prev_val_loss = 9999999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using class\n",
    "train_data = Audiobooks_Data_Reader('train', batch_size)\n",
    "val_data = Audiobooks_Data_Reader('val', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Mean loss: 0.615. Validation loss: 0.504. Validation accuracy: 76.00%\n",
      "Epoch 2. Mean loss: 0.471. Validation loss: 0.425. Validation accuracy: 85.00%\n",
      "Epoch 3. Mean loss: 0.422. Validation loss: 0.384. Validation accuracy: 85.00%\n",
      "Epoch 4. Mean loss: 0.397. Validation loss: 0.359. Validation accuracy: 84.00%\n",
      "Epoch 5. Mean loss: 0.381. Validation loss: 0.343. Validation accuracy: 84.00%\n",
      "Epoch 6. Mean loss: 0.370. Validation loss: 0.330. Validation accuracy: 84.00%\n",
      "Epoch 7. Mean loss: 0.362. Validation loss: 0.318. Validation accuracy: 84.00%\n",
      "Epoch 8. Mean loss: 0.355. Validation loss: 0.310. Validation accuracy: 84.00%\n",
      "Epoch 9. Mean loss: 0.350. Validation loss: 0.304. Validation accuracy: 85.00%\n",
      "Epoch 10. Mean loss: 0.346. Validation loss: 0.299. Validation accuracy: 86.00%\n",
      "Epoch 11. Mean loss: 0.342. Validation loss: 0.297. Validation accuracy: 86.00%\n",
      "Epoch 12. Mean loss: 0.340. Validation loss: 0.294. Validation accuracy: 86.00%\n",
      "Epoch 13. Mean loss: 0.337. Validation loss: 0.292. Validation accuracy: 86.00%\n",
      "Epoch 14. Mean loss: 0.335. Validation loss: 0.291. Validation accuracy: 85.00%\n",
      "Epoch 15. Mean loss: 0.333. Validation loss: 0.290. Validation accuracy: 85.00%\n",
      "Epoch 16. Mean loss: 0.332. Validation loss: 0.289. Validation accuracy: 85.00%\n",
      "Epoch 17. Mean loss: 0.330. Validation loss: 0.287. Validation accuracy: 85.00%\n",
      "Epoch 18. Mean loss: 0.329. Validation loss: 0.287. Validation accuracy: 86.00%\n",
      "Epoch 19. Mean loss: 0.327. Validation loss: 0.286. Validation accuracy: 86.00%\n",
      "Epoch 20. Mean loss: 0.326. Validation loss: 0.285. Validation accuracy: 86.00%\n",
      "Epoch 21. Mean loss: 0.325. Validation loss: 0.284. Validation accuracy: 86.00%\n",
      "Epoch 22. Mean loss: 0.324. Validation loss: 0.284. Validation accuracy: 86.00%\n",
      "End of training\n"
     ]
    }
   ],
   "source": [
    "# Make it learn\n",
    "\n",
    "# Create a loop for the epochs. e is a variable that automatically\n",
    "# starts from 0\n",
    "for e in range(max_epochs):\n",
    "    \n",
    "    \n",
    "    #TRAINING!!!!!!!!!\n",
    "    # Keep track of sum of batch losses in the epoch\n",
    "    epoch_loss = 0.\n",
    "    \n",
    "    # Iterate over the batches in this epoch using the CLASS\n",
    "    for input_batch, target_batch in train_data:\n",
    "        \n",
    "        # Run the optimization step and get the mean loss for \n",
    "        # this batch. Feed it with the inputs and targets we just\n",
    "        # got from the train set\n",
    "        _, batch_loss = sess.run([optimize, mean_loss],\n",
    "                                feed_dict={inputs:input_batch, targets:target_batch})\n",
    "        \n",
    "        # Increment the sum of the batch losses\n",
    "        epoch_loss += batch_loss\n",
    "        \n",
    "    # Average batch loss    \n",
    "    epoch_loss /= train_data.batch_count #this is the training loss\n",
    "    \n",
    "    \n",
    "    # VALIDATION!!!!!!!!!!\n",
    "    # Run without the optimization step (simply forward propagate)\n",
    "    val_loss = 0.\n",
    "    val_accuracy = 0.\n",
    "    \n",
    "    for input_batch, target_batch in val_data:\n",
    "        val_loss, val_accuracy = sess.run([mean_loss, accuracy],\n",
    "                                 feed_dict={inputs: input_batch, targets:target_batch})\n",
    "        \n",
    "    \n",
    "    # Print stats for each epoch\n",
    "    print('Epoch '+str(e+1)+\n",
    "          '. Mean loss: '+'{0:.3f}'.format(epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.3f}'.format(val_loss)+\n",
    "          '. Validation accuracy: '+'{0:.2f}'.format(val_accuracy * 100.)+'%')\n",
    "    \n",
    "    # Trigger early stopping if val_loss increases\n",
    "    if val_loss > prev_val_loss:\n",
    "        break\n",
    "        \n",
    "    # Store this epoch's val_loss to be used as prev_val_loss\n",
    "    prev_val_loss = val_loss\n",
    "    \n",
    "        \n",
    "# What does the inside of the 2nd for loop do?\n",
    "# 1. Loads 100 inputs and targets (batch_size=100)\n",
    "# 2. Optimizes the algorithm and calculates the batch loss\n",
    "# 3. Records the loss for the iteration\n",
    "# 4. Starts with the next 100 inputs and targets\n",
    "# 5. Stops when the training set is exhausted\n",
    "\n",
    "print('End of training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 81.25%\n"
     ]
    }
   ],
   "source": [
    "test_data = Audiobooks_Data_Reader('test')\n",
    "\n",
    "for input_batch, target_batch in test_data:\n",
    "    test_accuracy = sess.run([accuracy],\n",
    "                    feed_dict = {inputs:input_batch, targets:target_batch})\n",
    "    \n",
    "test_accuracy_percent = test_accuracy[0]*100\n",
    "\n",
    "print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3-TF1.0]",
   "language": "python",
   "name": "conda-env-python3-TF1.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
