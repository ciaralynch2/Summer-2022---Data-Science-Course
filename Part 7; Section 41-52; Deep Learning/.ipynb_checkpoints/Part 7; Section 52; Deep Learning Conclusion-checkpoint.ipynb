{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Training an algorithm involves four ingredients:\n",
    "- Data\n",
    "- Model\n",
    "- Objective Function\n",
    "- Optimiztion Algorithm\n",
    "\n",
    "We changed our model from simple linear or logistic models using activation functions to allow us to use hidden layers.\n",
    "\n",
    "We also examined the mathematics behind back propagation, and how to deal with overfitting.\n",
    "\n",
    "Then we learned to apply early learning techniques, and about the different types of initializations\n",
    "\n",
    "There are two very important takeaways:\n",
    "1. **Batching:** This gives us a significant speed improvement through using the Mini-batch gradient descent\n",
    "2. **Learning Rate:** The learning rate was equally important, after upgrading it with momentum and learning rate schedules, we reached ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's next in Machine Learning?\n",
    "Areas that would require further study:\n",
    "- Text processing\n",
    "- Smart Assistance\n",
    "- Self-driving cars\n",
    "- Facial recognition\n",
    "\n",
    "Tools used in these areas are **Convolutional Neural Networks**(CNNs) and **Recurrent Neural Networks** (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs\n",
    "\n",
    "**DeepMind** is a division of Alphabet Inc. (owners of Google) that is responsible for developing general purpose artificial intelligence technology. It is extremely productive and successful not only in the machine learning field (where they are supposedly the best in the world) but also in their *efforts to communicate* their findings to the world. \n",
    "\n",
    "### Quick Summary\n",
    "Recall in the MNIST example, we flattened the picture into a 784x1 vector, this way however we lost the spatial information of every pixel's neighbourhood of pixels. The 28th and 29th entry in our vector are actually on opposite sides of the picture.\n",
    "\n",
    "A CNN deals with the problem without flattening the image, instead it applies, for example a 5x5 kernel to every position of the image. Kernels are like weights.\n",
    "\n",
    "If we start from the top we would have 24 5x5 squares, and the same starting from the top left corneer and going right, we would have 24 5x5 squares. \n",
    "\n",
    "The next layer would be 24x24, which is the number of 5x5 sub-matrices, this is called **convolution**. This layer is known as a convolutional layer. The number of kernels you choose is a hyperparameter.\n",
    "\n",
    "Apart from convolution there is another main step - **pooling**. Most commonly, we would divide these 24x24 squares into multiple 2x2 squares without overlapping. Normally we will take (and keep) the largest m=number from the square as it is the **strongest detail**. This is how we reduce the dimensionality of the problem.\n",
    "\n",
    "What makes the issue slightly more complicated though is that most images have , which implies images have height and width, but also colour (i.e. depth). This would result in 3 convolutional layers and 3 pooling layers using the RGB scheme.\n",
    "\n",
    "If we repeat the process of convolution and pooling for long enough, we can reduce the dimensions to a vector containing one-hot encoded categories.\n",
    "\n",
    "### Uses, Advantages\n",
    "Used a lot in image/facial recognition.\n",
    "\\\n",
    "**Advantages:**\n",
    "- Spatial proximities are preserved\n",
    "- A detail is looked for everywhere in a photo, i.e. the object does not have to be in the centre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs\n",
    "\n",
    "Specifically designed for sequential data, e.g. stock trading, speak recognition and music. RNNs have memory.\n",
    "\n",
    "### Quick Summary\n",
    "Imagine we have an input layer, a hidden layer, and an output layer. We forward propagate, but we keep the information from the hidden layer. For the second sample, we forward propagate again but the hidden layer for the sample is a function of the new inputs, the updated weights, **and the hidden function from the previous iteration**.\n",
    "\n",
    "This means we need to know the weights of the hidden units too. \n",
    "\n",
    "Unrolling the RNN gives an extremely deep feedforward NN. This summary may make RNNs seem simple but they are very computationally expensive, due to the high number of layers and interconnection involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Neural Network Approaches\n",
    "\n",
    "### Discriminative Models\n",
    "Throughout the whole course we considered **discriminative models**. A discriminative model is one that uses an input and then provides a probability of whether an output is correct. \n",
    "\\\n",
    "Another instance of discriminative models are random forests. They are based on decision trees. The idea is that decision tree alone is bad at classification, they tend to overfit a lot. But a random forest combines decision trees and this forms a good classifier. Random forests are mainly used for classification.\n",
    "\n",
    "### Generative models\n",
    "These models don't give an output Y given X. The target is actually the **joint probability distribution** of X and Y, which carries more information. It goes from inputs to outputs and then outputs to inputs. This is useful for problems such as translators. \n",
    "\\\n",
    "Examples include; Hidden Markov models, Bayesian networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
