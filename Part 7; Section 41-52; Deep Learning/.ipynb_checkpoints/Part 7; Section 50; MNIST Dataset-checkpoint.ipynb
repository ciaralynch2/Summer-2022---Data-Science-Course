{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to approach the dataset\n",
    "- Each image is 28 x 28 pixels, it is on a greyscale, so we can think of the problem as a 28 x 28 matrix where input values are from 0 to 255\n",
    "- 0 = black, 255 = white\n",
    "- The approach for deep neural networks is to 'flatten' each image into a vector 784 x 1\n",
    "- Each pixel is an input in the input layer\n",
    "- There are 10 digits, so 10 classes, so 10 output units in the output layer\n",
    "- The output will then be compared to the targets, we will use one hot encoding for both the outputs and the targets\n",
    "- Since we would like to see the probability of a digit being correctly labelled we will use a softmax activation function for the output layer\n",
    "\n",
    "## Action Plan\n",
    "1. Prepare our data and preprocess it. Create training, validation, and test datasets\n",
    "2. Outline the model and choose the activation functions\n",
    "3. Set the appropriate advanced optimizers and the loss function\n",
    "4. Make it learn\n",
    "5. Test the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data, mnist_info = tfds.load(name = 'mnist', with_info = True, as_supervised = True)\n",
    "\n",
    "#tfds.load(name, with_info, as_supervised) loads a dataset from \n",
    "#tensorflow datasets \n",
    "#as_supervised=True loads the data in a 2-tuple structure \n",
    "#[input, target]\n",
    "#with_info=True provides a tuple containing info about version\n",
    "#features, no. of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_data['train'], mnist_data['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val = 0.1*mnist_info.splits['train'].num_examples\n",
    "num_val = tf.cast(num_val, tf.int64)\n",
    "\n",
    "# Want the validation set to be 10% of the training set\n",
    "# So the number of validation samples is 0.1 times the number\n",
    "# of training samples to the nearest integer\n",
    "\n",
    "num_test = mnist_info.splits['test'].num_examples\n",
    "num_test = tf.cast(num_test, tf.int64)\n",
    "# Want the number of samples in the test set to be easily\n",
    "# accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally we'd like to scale our data in some way to make the\n",
    "# result more numerically stable (e.g. inputs between 0 & 1)\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "# We want all values to be floats so we cast the image to float\n",
    "# Next we scale it, since the images contain values from 0 to\n",
    "# 255, we can divide the values by 255 to get values between 0\n",
    "# and 1\n",
    "\n",
    "# image /= 255. the dot at the end signifies we want the result\n",
    "# to be a float\n",
    "\n",
    "# dataset.map(function) applies a custom transformation to a\n",
    "# given dataset. It takes as input a function which determines\n",
    "# the transformation\n",
    "scaled_train_val_data = mnist_train.map(scale)\n",
    "\n",
    "scaled_test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "\n",
    "# We shuffle the data so batching works effectively\n",
    "# When we are dealing with enormous datasets, we can't shuffle\n",
    "# all the data at once, so we take 10000 samples shuffle them and\n",
    "# then take the next 10000\n",
    "\n",
    "# If buffer size>= num_samples shuffling will happen all at\n",
    "# once (uniformly)\n",
    "\n",
    "shuffle_train_val_data = scaled_train_val_data.shuffle(buffer_size)\n",
    "\n",
    "val_data = shuffle_train_val_data.take(num_val)\n",
    "# val_data = 10% of train data which we stored in num_val\n",
    "# We can use .take() to extract that many samples\n",
    "\n",
    "train_data = shuffle_train_val_data.skip(num_val)\n",
    "# We can get the train_data by extracting all elements but the\n",
    "# first x validation samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# dataset.batch(batch_size) is a method that combines the \n",
    "# consecutive elements of a dataset into batches\n",
    "\n",
    "train_data = train_data.batch(batch_size)\n",
    "\n",
    "# We don't need to batch the val_data as we don't back \n",
    "# propagate that, however the model expects the val_data in \n",
    "# batch form too\n",
    "\n",
    "val_data = val_data.batch(num_val)\n",
    "# This indicates that the model should take the whole val_data\n",
    "# at once\n",
    "\n",
    "test_data = scaled_test_data.batch(num_test)\n",
    "\n",
    "val_inputs, val_targets = next(iter(val_data))\n",
    "# iter() creates an object which can be iterated one element\n",
    "# at a time (e.g. in a for loop or while loop)\n",
    "# next() loads the next element of an iterable object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### Outline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_size = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = (28, 28, 1)),\n",
    "    tf.keras.layers.Dense(hidden_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# tf.keras.layers.Flatten(original shape) transforms a tensor\n",
    "# into a vector\n",
    "# tf.keras.layers.Dense(output size, activation) takes the \n",
    "# inputs, provided the model and calculates the dot product of \n",
    "# the inputs and the weights and adds the bias. This is also \n",
    "# where we can apply an activation function\n",
    "# relu is a good activation function for the mnist set\n",
    "\n",
    "# The above format is how we stack layers we have two hidden \n",
    "# layers and the output layer, we have assumed the hidden \n",
    "# layers are the same size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# These strings are not case sensitive\n",
    "# We want to use cross-entropy as the loss function here but\n",
    "# TensorFlow has 3 cross-entropy functions:\n",
    "# binary_crossentropy refers to the case where we have binary\n",
    "# encoding\n",
    "# categorical_crossentropy expects that you've one-hot encoded\n",
    "# the targets\n",
    "# sparse_categorical_crossentropy applies one-hot encoding\n",
    "\n",
    "# We can include metrics that we wish to calculate throughout\n",
    "# the training and testing processes, typically that's the \n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "#### What happens inside each epoch\n",
    "1. At the beginning of each epoch the training loss will be set to 0\n",
    "2. The algorithm will iterate iver a preset number of batches all from train_data\n",
    "3. The weights and biases will be updated as many times as there are batches\n",
    "4. We will get a value for the loss function, indicating how the training is going\n",
    "5. We will also see a training accuracy\n",
    "6. At the end of each epoch, the algorithm will forward propagate the whole validation set\n",
    "\n",
    "**We keep an eye on the validation loss to make sure no overfitting occurs. The validation accuracy is the true accuracy of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "540/540 - 3s - loss: 0.3309 - accuracy: 0.9050 - val_loss: 0.1703 - val_accuracy: 0.9500 - 3s/epoch - 6ms/step\n",
      "Epoch 2/100\n",
      "540/540 - 2s - loss: 0.1404 - accuracy: 0.9592 - val_loss: 0.1237 - val_accuracy: 0.9625 - 2s/epoch - 4ms/step\n",
      "Epoch 3/100\n",
      "540/540 - 2s - loss: 0.0997 - accuracy: 0.9699 - val_loss: 0.0920 - val_accuracy: 0.9720 - 2s/epoch - 4ms/step\n",
      "Epoch 4/100\n",
      "540/540 - 2s - loss: 0.0762 - accuracy: 0.9769 - val_loss: 0.0811 - val_accuracy: 0.9740 - 2s/epoch - 4ms/step\n",
      "Epoch 5/100\n",
      "540/540 - 2s - loss: 0.0613 - accuracy: 0.9816 - val_loss: 0.0692 - val_accuracy: 0.9783 - 2s/epoch - 4ms/step\n",
      "Epoch 6/100\n",
      "540/540 - 2s - loss: 0.0508 - accuracy: 0.9840 - val_loss: 0.0510 - val_accuracy: 0.9845 - 2s/epoch - 4ms/step\n",
      "Epoch 7/100\n",
      "540/540 - 2s - loss: 0.0414 - accuracy: 0.9875 - val_loss: 0.0461 - val_accuracy: 0.9860 - 2s/epoch - 4ms/step\n",
      "Epoch 8/100\n",
      "540/540 - 2s - loss: 0.0356 - accuracy: 0.9888 - val_loss: 0.0467 - val_accuracy: 0.9842 - 2s/epoch - 4ms/step\n",
      "Epoch 9/100\n",
      "540/540 - 2s - loss: 0.0308 - accuracy: 0.9904 - val_loss: 0.0432 - val_accuracy: 0.9855 - 2s/epoch - 4ms/step\n",
      "Epoch 10/100\n",
      "540/540 - 2s - loss: 0.0260 - accuracy: 0.9921 - val_loss: 0.0403 - val_accuracy: 0.9875 - 2s/epoch - 4ms/step\n",
      "Epoch 11/100\n",
      "540/540 - 2s - loss: 0.0226 - accuracy: 0.9931 - val_loss: 0.0292 - val_accuracy: 0.9902 - 2s/epoch - 4ms/step\n",
      "Epoch 12/100\n",
      "540/540 - 2s - loss: 0.0184 - accuracy: 0.9941 - val_loss: 0.0226 - val_accuracy: 0.9927 - 2s/epoch - 4ms/step\n",
      "Epoch 13/100\n",
      "540/540 - 2s - loss: 0.0159 - accuracy: 0.9953 - val_loss: 0.0187 - val_accuracy: 0.9942 - 2s/epoch - 4ms/step\n",
      "Epoch 14/100\n",
      "540/540 - 2s - loss: 0.0157 - accuracy: 0.9950 - val_loss: 0.0210 - val_accuracy: 0.9925 - 2s/epoch - 4ms/step\n",
      "Epoch 15/100\n",
      "540/540 - 2s - loss: 0.0128 - accuracy: 0.9960 - val_loss: 0.0171 - val_accuracy: 0.9935 - 2s/epoch - 4ms/step\n",
      "Epoch 16/100\n",
      "540/540 - 2s - loss: 0.0104 - accuracy: 0.9968 - val_loss: 0.0126 - val_accuracy: 0.9955 - 2s/epoch - 4ms/step\n",
      "Epoch 17/100\n",
      "540/540 - 2s - loss: 0.0118 - accuracy: 0.9961 - val_loss: 0.0226 - val_accuracy: 0.9918 - 2s/epoch - 4ms/step\n",
      "Epoch 18/100\n",
      "540/540 - 2s - loss: 0.0119 - accuracy: 0.9958 - val_loss: 0.0201 - val_accuracy: 0.9937 - 2s/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20c35500640>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "model.fit(train_data,\n",
    "          epochs = num_epochs,\n",
    "          callbacks = [early_stopping],\n",
    "          validation_data = (val_inputs, val_targets),\n",
    "          verbose=2\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1074 - accuracy: 0.9773\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.11. Test accuracy: 97.73%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After we test the model, conceptually, we are no longer allowed to change it\n",
    "- Getting a test accuracy close to the validation accuracy shows we did not overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
