{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "1. Balance the dataset\n",
    "2. Divide the dataset into training, validation, and test\n",
    "3. Save the data in a tensor friendly format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the dataset\n",
    "Say we have a dataset with pictures of cats and dogs where 90% of the pictures are cats. \n",
    "\n",
    "Our machine learning algorithm may find that the best output to increase accuracy is to say all pictures are cats, as this gives a 90% accuracy. This is not ideal.\n",
    "\n",
    "We refer to the initial probability of picking a photo of some class as a prior. The priors in the above example are 0.9 for cats and 0.1 for dogs. The priors are balanced when 50% of the photos are cats and 50% are dogs. The example we have shows unbalanced priors. Unbalanced priors cause the case described above.\n",
    "\n",
    "In our business case by exploring the targets we can see that most customers did not convert. So we must balance the dataset before we proceed. This is done by counting the number of target 1s and matching the same number of 0s to them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "# We load preprocessing as we will use it to standardise inputs\n",
    "# We almost always standardise inputs as it usually greatly improves\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.loadtxt('Audiobooks_data.csv', delimiter = ',')\n",
    "\n",
    "# A delimiter is a sequence of one or more characters used to specify\n",
    "# the boundary between separate independent regions in plain text or\n",
    "# other data streams\n",
    "\n",
    "unscaled_inputs = raw_data[:, 1:-1]\n",
    "# the inputs are all columns in the csv except the first and last\n",
    "# the first column is the arbitrarily chosen ID \n",
    "\n",
    "targets_all = raw_data[:,-1]\n",
    "# the last column in the csv is the targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the dataset\n",
    "1. We will count the number of targets that are 1s (as we know there are more 0s)\n",
    "2. We will keep as many 0s as 1s (and we will delete the others) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets_all))\n",
    "# Since the targets can only take values 0 and 1, by summing all the\n",
    "# targets, we see how many 1s there are\n",
    "\n",
    "zero_counter = 0\n",
    "ind_remove = []\n",
    "\n",
    "# This loop will keep as many 0s as 1s and then put the index of any\n",
    "# extra 0s in the ind_remove list for us to remove later\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_counter += 1\n",
    "        if zero_counter > num_one_targets:\n",
    "            ind_remove.append(i)\n",
    "            \n",
    "# targets_all.shape[0] gives the number of rows in the csv \n",
    "# (i.e. number of targets)\n",
    "\n",
    "# the zero_counter will increase every time there is a zero in the \n",
    "# targets, eventually the zero_counter will have counted all 0s in\n",
    "# the targets. But, once there are more 0s than 1s, the index of the\n",
    "# extra 0s is put in the ind_remove list for us to remove later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_inputs_ep = np.delete(unscaled_inputs, ind_remove, axis=0)\n",
    "targets_ep = np.delete(targets_all, ind_remove, axis=0)\n",
    "\n",
    "# np.delete(array, object to delete, axis) is a method that deletes\n",
    "# an object along an axis, in this case it deletes all rows with an\n",
    "# index in ind_remove\n",
    "\n",
    "# ep is for 'equal priors' which is what we wanted to achieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = pp.scale(unscaled_inputs_ep)\n",
    "\n",
    "# preprocessing.scale(x) is a method that standardises an array along\n",
    "# an axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is important for effective batching\n",
    "\n",
    "shuffled_ind = np.arange(scaled_inputs.shape[0])\n",
    "# np.arange([start], stop) is a method that returns an array of \n",
    "# evenly spaced values within a given interval\n",
    "# In this case shuffled_ind = an array of numbers from 0 to the \n",
    "# length of scaled_inputs (i.e. all row indices)\n",
    "\n",
    "np.random.shuffle(shuffled_ind)\n",
    "# This shuffles the indices\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_ind]\n",
    "# scaled_inputs[shuffled_ind] returns the array with shuffled values\n",
    "\n",
    "shuffled_targets = targets_ep[shuffled_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1774.0 3579 0.4956691813355686\n",
      "232.0 447 0.5190156599552572\n",
      "231.0 448 0.515625\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "train_size = int(0.8*samples_count)\n",
    "val_size = int(0.1*samples_count)\n",
    "test_size = samples_count - train_size - val_size\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_size]\n",
    "train_targets = shuffled_targets[:train_size]\n",
    "\n",
    "val_inputs = shuffled_inputs[train_size : train_size + val_size]\n",
    "val_targets = shuffled_targets[train_size : train_size + val_size]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_size + val_size :]\n",
    "test_targets = shuffled_targets[train_size + val_size:]\n",
    "\n",
    "# We want to check we have not just balanced the dataset but also the\n",
    "# training, validation, and test datasets\n",
    "\n",
    "print(np.sum(train_targets), train_size, np.sum(train_targets)/train_size)\n",
    "print(np.sum(val_targets), val_size, np.sum(val_targets)/val_size)\n",
    "print(np.sum(test_targets), test_size, np.sum(test_targets)/test_size)\n",
    "\n",
    "# They are all approximately 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the 3 datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_val', inputs=val_inputs, targets=val_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the machine learning algorithm\n",
    "\n",
    "Let's discuss our net quickly. We have 10 input nodes, 2 hidden layers, and 2 output nodes. Each hidden layer has 50 nodes, this provides enough complexity, so we expect the algorithm to be much more sophisticated than a linear or logistic model. \n",
    "\\\n",
    "However we don't want to put too many nodes in the hidden layers initially so we can complete the learning faster, so 50 is a good starting number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# numpy is also needed but was imported earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "# Recall we save npz in 2-tuple format [inputs, targets]\n",
    "\n",
    "train_inputs = npz['inputs'].astype(float)\n",
    "train_targets = npz['targets'].astype(int)\n",
    "\n",
    "# We expect all inputs to be floats, and we expect all targets to be \n",
    "# integers. This is good practice even if we know we saved the \n",
    "# targets as integers and not boolean values or floats\n",
    "\n",
    "npz = np.load('Audiobooks_data_val.npz')\n",
    "val_inputs = npz['inputs'].astype(float)\n",
    "val_targets = npz['targets'].astype(int)\n",
    "\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "test_inputs = npz['inputs'].astype(float)\n",
    "test_targets = npz['targets'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Outline, optimizers, loss, early stopping, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 1s - loss: 0.5475 - accuracy: 0.7485 - val_loss: 0.4224 - val_accuracy: 0.8658 - 721ms/epoch - 20ms/step\n",
      "Epoch 2/100\n",
      "36/36 - 0s - loss: 0.3646 - accuracy: 0.8771 - val_loss: 0.3308 - val_accuracy: 0.8792 - 94ms/epoch - 3ms/step\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.3151 - accuracy: 0.8852 - val_loss: 0.3034 - val_accuracy: 0.8904 - 91ms/epoch - 3ms/step\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.2938 - accuracy: 0.8919 - val_loss: 0.2896 - val_accuracy: 0.8904 - 90ms/epoch - 3ms/step\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.2807 - accuracy: 0.8944 - val_loss: 0.2853 - val_accuracy: 0.8993 - 92ms/epoch - 3ms/step\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.2723 - accuracy: 0.8983 - val_loss: 0.2734 - val_accuracy: 0.8971 - 93ms/epoch - 3ms/step\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.2646 - accuracy: 0.8994 - val_loss: 0.2696 - val_accuracy: 0.9016 - 91ms/epoch - 3ms/step\n",
      "Epoch 8/100\n",
      "36/36 - 0s - loss: 0.2603 - accuracy: 0.9022 - val_loss: 0.2637 - val_accuracy: 0.9016 - 92ms/epoch - 3ms/step\n",
      "Epoch 9/100\n",
      "36/36 - 0s - loss: 0.2559 - accuracy: 0.9028 - val_loss: 0.2618 - val_accuracy: 0.9038 - 91ms/epoch - 3ms/step\n",
      "Epoch 10/100\n",
      "36/36 - 0s - loss: 0.2501 - accuracy: 0.9030 - val_loss: 0.2568 - val_accuracy: 0.9060 - 97ms/epoch - 3ms/step\n",
      "Epoch 11/100\n",
      "36/36 - 0s - loss: 0.2477 - accuracy: 0.9056 - val_loss: 0.2551 - val_accuracy: 0.8993 - 102ms/epoch - 3ms/step\n",
      "Epoch 12/100\n",
      "36/36 - 0s - loss: 0.2460 - accuracy: 0.9047 - val_loss: 0.2512 - val_accuracy: 0.9038 - 99ms/epoch - 3ms/step\n",
      "Epoch 13/100\n",
      "36/36 - 0s - loss: 0.2438 - accuracy: 0.9072 - val_loss: 0.2543 - val_accuracy: 0.9038 - 94ms/epoch - 3ms/step\n",
      "Epoch 14/100\n",
      "36/36 - 0s - loss: 0.2429 - accuracy: 0.9100 - val_loss: 0.2508 - val_accuracy: 0.9105 - 93ms/epoch - 3ms/step\n",
      "Epoch 15/100\n",
      "36/36 - 0s - loss: 0.2384 - accuracy: 0.9109 - val_loss: 0.2502 - val_accuracy: 0.9060 - 99ms/epoch - 3ms/step\n",
      "Epoch 16/100\n",
      "36/36 - 0s - loss: 0.2365 - accuracy: 0.9106 - val_loss: 0.2487 - val_accuracy: 0.9060 - 89ms/epoch - 2ms/step\n",
      "Epoch 17/100\n",
      "36/36 - 0s - loss: 0.2332 - accuracy: 0.9128 - val_loss: 0.2524 - val_accuracy: 0.9083 - 89ms/epoch - 2ms/step\n",
      "Epoch 18/100\n",
      "36/36 - 0s - loss: 0.2361 - accuracy: 0.9134 - val_loss: 0.2523 - val_accuracy: 0.9060 - 90ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x172d3ac6700>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_size = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_size, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(hidden_size, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 100\n",
    "max_epochs = 100\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "model.fit(train_inputs,\n",
    "          train_targets,\n",
    "          batch_size = batch_size,\n",
    "          epochs = max_epochs,\n",
    "          callbacks = [early_stopping],\n",
    "          validation_data = (val_inputs, val_targets),\n",
    "          verbose = 2\n",
    "         )\n",
    "\n",
    "# We no longer need the layers.Flatten() as we have preprocessed the\n",
    "# data properly so it is already in this format\n",
    "\n",
    "# Indicating the batch size in .fit() will automatically batch the\n",
    "# data\n",
    "\n",
    "# By default callbacks.EarlyStopping() will monitor the validation\n",
    "# loss and stop the training process the first time the validation\n",
    "# loss starts increasing\n",
    "\n",
    "# EarlyStopping(patience) configures the early stopping mechanism of\n",
    "# the algorithm. 'patience' lets us decide how many consecutive\n",
    "# increases we can tolerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2136 - accuracy: 0.9241\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.21. Test accuracy: 92.41%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
